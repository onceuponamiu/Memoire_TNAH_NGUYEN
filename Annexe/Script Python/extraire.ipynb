{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PElqB0uNsXyv",
        "outputId": "f356c2c7-dc5d-4372-c137-b458dfed2151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (10.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ],
      "source": [
        "pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pqxEvF13JND",
        "outputId": "cee8e10f-ae4b-4d14-cff1-ca216a731d50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXtxoEEJ1rvd"
      },
      "source": [
        "#  Importer les bibliothèques nécessaires\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZXuHZJCossxP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import re\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmvbwWrH1uEv"
      },
      "source": [
        "# Extraire Titre et Texte (Sujet de divers et Articles de loi)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo13uXWmKofj"
      },
      "outputs": [],
      "source": [
        "# Supprimer des mots ne souhaites pas\n",
        "def remove_unwanted_terms(text):\n",
        "    unwanted_terms = [\n",
        "        'Mme la présidente.',\n",
        "        'Mme le présidente.',\n",
        "        'M. le président.',\n",
        "        'Mme le président.',\n",
        "        'Mme la président.'\n",
        "    ]\n",
        "    for term in unwanted_terms:\n",
        "        text = text.replace(term, '')\n",
        "    return text.strip()\n",
        "\n",
        "\"\"\"\n",
        "Cette fonction va : lire le fichier PDF, parcourir chaque page pour extraire les titres (Titre) et le texte (Texte)\n",
        "Titre : Filtrera les parties en gras car le titre est en gras\n",
        "Texte : Recherchera les sections en gras avec les mots suivants : Article, Avant l’article, Après l’article\n",
        "\"\"\"\n",
        "def extract_Titre_Texte(pdf_path):\n",
        "    data = []\n",
        "    expected_number = 1  # Variable pour suivre le numéro d'ordre actuel\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number in range(len(pdf.pages)):\n",
        "            current_line = \"\"\n",
        "            page = pdf.pages[page_number]\n",
        "            chars = page.chars  # chars là một cái dictionary # chars est un dictionnaire\n",
        "\n",
        "            for char in chars:\n",
        "                if \"Bold\" in char[\"fontname\"]: # Si \"Bold\" est présent dans le nom de la police -> C'est une section en gras\n",
        "\n",
        "                    current_line += char[\"text\"] # Le texte ici est composé de caractères séparés (a b c 1 2 -)\n",
        "                    # Si ce n'est pas en gras et que ce n'est pas vide -> La section en gras a été complètement récupérée\n",
        "                     # current_line devient une phrase complète en gras\n",
        "                elif current_line.strip():\n",
        "                    match = re.match(r\"(\\d+)\\s+([A-Za-zÀ-Ýa-zà-ý\\s:’.,;?!()\\[\\]«»\\-]*)(?:\\s*(Mme la présidente\\.|Mme le présidente\\.|M\\. le président\\.|Mme le président\\.|Mme la président\\.))?$\", current_line.strip())\n",
        "                    if match and int(match.group(1)) == expected_number:\n",
        "                        # Si le premier numéro correspond au numéro d'ordre attendu\n",
        "                        expected_number += 1  # Tăng số thứ tự cho dòng tiếp theo # Incrémenter le numéro d'ordre pour la ligne suivante\n",
        "                        title_text = f\"{match.group(2).strip()}\"\n",
        "                        data.append((page_number + 1, current_line.strip(), title_text, \"\"))\n",
        "                    else:\n",
        "                        data.append((page_number + 1, current_line.strip(), None, \"\"))\n",
        "                    current_line = \"\"\n",
        "\n",
        "        # Après la boucle for, traiter la dernière ligne de la page (souvent le numéro de page)\n",
        "            if current_line.strip():\n",
        "                # 1 ou plusieurs chiffres (\\d+), (1 ou plusieurs espaces (\\s+)) (groupe 1)\n",
        "                # ... (groupe 2)\n",
        "                match = re.match(r\"(\\d+)\\s+([A-Za-zÀ-Ýa-zà-ý\\s:’.,;?!()\\[\\]«»\\-]*)(?:\\s*(Mme la présidente\\.|Mme le présidente\\.|M\\. le président\\.|Mme le président\\.|Mme la président\\.))?$\", current_line.strip())\n",
        "                if match and int(match.group(1)) == expected_number:\n",
        "                    expected_number += 1\n",
        "                    title_text = f\"{match.group(2).strip()} \"\n",
        "                    data.append((page_number + 1, current_line.strip(), title_text, \"\"))\n",
        "                else:\n",
        "                    data.append((page_number + 1, current_line.strip(), None, \"\"))\n",
        "\n",
        "    # Supprimer les termes indésirables à la fin du texte (Mme la présidente) du TITRE\n",
        "        for i in range(len(data)):\n",
        "            if data[i][2] is not None:\n",
        "                data[i] = (data[i][0], data[i][1], remove_unwanted_terms(data[i][2]), data[i][3])\n",
        "\n",
        "# Remplacer les valeurs None ou vides par la valeur du Titre précédent, et les modifier si un autre Titre est trouvé\n",
        "    last_titre_text = None\n",
        "    for i in range(len(data)):\n",
        "        if data[i][2] is not None:\n",
        "            last_titre_text = data[i][2]\n",
        "        elif last_titre_text is not None:\n",
        "            data[i] = (data[i][0], data[i][1], last_titre_text, data[i][3])\n",
        "\n",
        "  # Si les sections en gras contiennent des termes comme : Après l’article, Article, Avant l’article, les ajouter à la liste (appelée thème)\n",
        "    for i in range(len(data)):\n",
        "        if data[i][1] and (\"Après l’article\" in data[i][1] or \"Article\" in data[i][1] or \"Avant l’article\" in data[i][1]):\n",
        "            data[i] = (data[i][0], data[i][1], data[i][2], data[i][1])\n",
        "\n",
        "\n",
        "  # Remplacer les valeurs vides ou None par la valeur du Texte précédent, et les modifier si un autre Texte est trouvé\n",
        "    last_texte_text = \"\"\n",
        "    for i in range(len(data)):\n",
        "        if data[i][3] != \"\":\n",
        "            last_texte_text = data[i][3]\n",
        "        else:\n",
        "            data[i] = (data[i][0], data[i][1], data[i][2], last_texte_text)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G87yJg8Q12ga"
      },
      "source": [
        "# Extraire Thème ( Nature de textes de loi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXd7V-AQKuNK"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "# Cette fonction va traiter le thème (Nature de texte) : Ce sera des sections de texte en majuscules mais non en gras, contenant les deux termes suivants : \"PROPOSITION DE LOI\" ou \"PROJET DE LOI\"\n",
        "'''\n",
        "def extract_topic(pdf_path, data):\n",
        "    topic_lines = []\n",
        "\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page_number in range(len(pdf.pages)):\n",
        "            current_line = \"\"\n",
        "            page = pdf.pages[page_number]\n",
        "            chars = page.chars\n",
        "\n",
        "            for char in chars:\n",
        "                if \"Bold\" not in char[\"fontname\"]:\n",
        "                    current_line += char[\"text\"]\n",
        "\n",
        "            # Verifier pour mot clés \"PROPOSITION DE LOI\" ou \"PROJET DE LOI\"\n",
        "            if \"PROPOSITION DE LOI\" in current_line or \"PROJET DE LOI\" in current_line:\n",
        "                topic_lines.append((page_number + 1, current_line))\n",
        "\n",
        "    theme_lines = {}\n",
        "    for page, text in topic_lines:\n",
        "        # Regex pour \"PROPOSITION DE LOI\"\n",
        "        # PROPOSITION DE LOI ne doit être attaché à aucun autre mot\n",
        "        pattern_proposition = r'\\b[A-ZÀ-Ý\\s\\.,;’!?()_-]*PROPOSITION DE LOI[A-ZÀ-Ý\\s\\.,;’!?()_-]*\\b'\n",
        "        matches_proposition = re.findall(pattern_proposition, text, re.DOTALL) # Trouver toutes les chaînes correspondantes, y compris les cas avec \\n, avec re.\n",
        "\n",
        "        # Regex pour \"PROJET DE LOI\"\n",
        "        pattern_projet = r'\\b[A-ZÀ-Ý\\s\\.,;’!?()_-]*PROJET DE LOI[A-ZÀ-Ý\\s\\.,;’!?()_-]*\\b'\n",
        "        matches_projet = re.findall(pattern_projet, text, re.DOTALL)\n",
        "\n",
        "        # Combine results # Combiner les résultats\n",
        "        matches = matches_proposition + matches_projet\n",
        "\n",
        "        if matches:\n",
        "            for match in matches:\n",
        "                 # Remplacer tous les espaces multiples par un seul espace et supprimer les espaces en début et en fin de chaîne\n",
        "                cleaned_match = re.sub(r'\\s+', ' ', match.strip())\n",
        "                #  supprimer les caractères spéciaux au début et à la fin\n",
        "\n",
        "                # ^ au début, $ à la fin\n",
        "                cleaned_match = re.sub(r'^[\\s\\.,;’!?()_-]+|[\\s\\.,;’!?()_-]+$', '', cleaned_match)\n",
        "                words = cleaned_match.split()\n",
        "\n",
        "                # Si le mot existe et que le dernier élément a une longueur < 2 : alors supprimer\n",
        "                while words and len(words[-1]) < 2:\n",
        "                    words.pop()\n",
        "                cleaned_match = ' '.join(words)\n",
        "                theme_lines[page] = cleaned_match\n",
        "\n",
        "\n",
        "    # Ajouter Thème à bold_lines pour chaque page\n",
        "    for i in range(len(data)):\n",
        "        page_number = data[i][0]   # Si page_number correspond -> transférer\n",
        "        if page_number in theme_lines:\n",
        "            data[i] = (data[i][0], data[i][1], data[i][2], data[i][3], theme_lines[page_number])\n",
        "\n",
        "# Remplacer les valeurs vides par la valeur du thème précédent\n",
        "    last_topic_text = \"\"\n",
        "    for i in range(len(data)):\n",
        "        if len(data[i]) > 4 and data[i][4] != \"\":\n",
        "            last_topic_text = data[i][4]\n",
        "        else:\n",
        "            data[i] = (data[i][0], data[i][1], data[i][2], data[i][3], last_topic_text)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALZoKCtT16Vp"
      },
      "source": [
        "# Extraire nom des intervenants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwWUnW82KzUt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_nom_intervenants(data):\n",
        "    # Préfixe\n",
        "    prefixes = ['Mme le', 'Mme la', 'M. le', 'M.', 'Mme']\n",
        "    extracted_nom_intervenants = []\n",
        "    page_values = {}\n",
        "\n",
        "    for page_number, text,titre, texte, topic in data:\n",
        "    # Vérifier si le texte est un nombre, si c'est un nombre -> numéro de page\n",
        "        if text.isdigit():\n",
        "            page_values[page_number] = text  # dic ( key: page_number)-> values : numéro de page\n",
        "\n",
        "    for page_number, text, titre, texte, topic in data:\n",
        "        for prefix in prefixes:\n",
        "            if prefix in text:\n",
        "                start_index = text.find(prefix)   # Trouver l'index commence\n",
        "                end_index = start_index + len(prefix) # Trouver l'index fin de préfixe\n",
        "                for i in range(end_index, len(text)):\n",
        "                    if text[i] in ['.', ',']:\n",
        "                        end_index = i + 1  # l'index fin de nom\n",
        "                        break\n",
        "                page_value = page_values.get(page_number, \"\") # extraire values pour page_number(key)\n",
        "                extracted_nom_intervenants.append((text[start_index:end_index].strip(), page_value, titre, texte, topic))\n",
        "                break\n",
        "\n",
        "    return extracted_nom_intervenants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c09td4hs3d1R"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbZ9GWDR1-pT"
      },
      "source": [
        "# Appeler et traiter en tant que DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFN3a_ypKl-I"
      },
      "outputs": [],
      "source": [
        "def process_file(pdf_path,date):\n",
        "\n",
        "    data = extract_Titre_Texte(pdf_path)\n",
        "    data = extract_topic(pdf_path, data)\n",
        "    data = extract_nom_intervenants(data)\n",
        "    df = pd.DataFrame(data, columns=['Nom_intervenants', 'Num_page','Titre','Texte','Thème'])\n",
        "    if not df.empty:\n",
        "        # Trouver l'index correspondant aux noms\n",
        "        first_president_row = df[(df['Nom_intervenants'] == 'Mme la présidente.') |\n",
        "                                 (df['Nom_intervenants'] == 'Mme le présidente.') |\n",
        "                                 (df['Nom_intervenants'] == 'M. le président.') |\n",
        "                                 (df['Nom_intervenants'] == 'Mme le président.') |\n",
        "                                 (df['Nom_intervenants'] == 'Mme la président.')].index[0]  # .index: renvoie les indices des lignes qui satisfont au moins une des conditions spécifiées.\n",
        "        # Prendre à partir de cet index jusqu'à la fin\n",
        "        df = df[first_president_row:]\n",
        "        df['Texte'] = df['Texte'].apply(remove_unwanted_terms)\n",
        "        df['Numero'] = range(1, len(df) + 1)\n",
        "        df['Date_seances'] = date\n",
        "        df = df[['Numero', 'Nom_intervenants', 'Date_seances','Num_page','Titre','Texte','Thème']]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4INVeqxP2asA"
      },
      "source": [
        "# Extraire les informations de date, appeler la fonction pour traiter les données et les enregistrer dans un fichier CSV\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_tRrd1d3k3k"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/Tieng_Phap\"\n",
        "output_dir = \"/content/drive/MyDrive/French_output_date_npage_part13\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "pdf_files = [f for f in os.listdir(data_dir) if f.endswith('.pdf')]\n",
        "french_months = [\"janvier\", \"février\", \"mars\", \"avril\", \"mai\", \"juin\", \"juillet\", \"août\", \"septembre\", \"octobre\", \"novembre\", \"décembre\"]\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    pdf_path = os.path.join(data_dir, pdf_file)\n",
        "    filename = os.path.splitext(pdf_file)[0]\n",
        "    year, month, day = filename[1:5], filename[5:7], filename[7:9]\n",
        "    month_name = french_months[int(month) - 1]  # Convertir mois de numero à mois en français\n",
        "    date = f\"{day} {month_name} {year}\"\n",
        "    df = process_pdf_file(pdf_path, date)\n",
        "\n",
        "    if not df.empty:\n",
        "        csv_file_name = f\"{filename}.csv\"\n",
        "        csv_file_path = os.path.join(output_dir, csv_file_name)\n",
        "        df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Processed and saved: {csv_file_name}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6UHIGPp5Has"
      },
      "source": [
        "# Fusionner les fichiers par date -> Un fichier unique:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAW0l9_v3k7D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Definier les chemins\n",
        "input_dir = \"/content/drive/MyDrive/French_output_date_npage_part13\"\n",
        "output_folder = '/content/drive/MyDrive/French_Outpu_22_9/data'\n",
        "output_file = 'combined_df.csv'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Obtenir la liste de tous les fichiers CSV dans le répertoire\n",
        "csv_files = [f for f in os.listdir(input_dir) if f.endswith('.csv')]\n",
        "\n",
        "# Créer une liste pour stocker les DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Lire et fusionner les fichiers CSV\n",
        "for file in csv_files:\n",
        "    file_path = os.path.join(input_dir, file)\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "combined_df.to_csv(f'{output_folder}/{output_file}', index=False,encoding='utf-8-sig')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vP1BjKqa90oI"
      },
      "source": [
        "# Vérifier s'il y a des noms incorrects\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlpI096B929v"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "target_strings = ['Mme le', 'Mme la', 'M. le', 'M.', 'Mme']\n",
        "\n",
        "# Créer un modèle regex pour correspondre exactement aux caractères dans la liste\n",
        "pattern = r'^(' + '|'.join(map(re.escape, target_strings)) + r')$'\n",
        "\n",
        "# Lọc các hàng trong DataFrame\n",
        "filtered_df = combined_df[combined_df['Nom_intervenants'].str.match(pattern)]\n",
        "filtered_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OMyQdpo-6HM"
      },
      "source": [
        "# Filtrer par nom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSoTiLwp-4tI",
        "outputId": "ea0ac9d0-5f37-489f-f012-0361c7731c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(456,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# fichier combined est fichier groupé tous les files qui classé par date de séances\n",
        "combined_df = pd.read_csv(\"/content/drive/MyDrive/French_Outpu_22_9/data/combined_df.csv\")\n",
        "output_dir = \"/content/drive/MyDrive/French_Outpu_22_9/original_data\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Supprimer les virgules ou les points à la fin de 'Nom_intervenants'\n",
        "combined_df['Nom_intervenants'] = combined_df['Nom_intervenants'].str.rstrip(',.')\n",
        "\n",
        "# Supprimer les éléments dans 'unwanted_names'\n",
        "unwanted_names = ['Mme le présidente', 'M. le président', 'Mme le président', 'Mme la président','Mme le présidente.', 'M. le président.', 'Mme le président.', 'Mme la président','Mme la présidente']\n",
        "combined_df = combined_df[~combined_df['Nom_intervenants'].isin(unwanted_names)]\n",
        "\n",
        "# Extraire chaque élément un par un\n",
        "unique_names = combined_df['Nom_intervenants'].unique()\n",
        "print(unique_names.shape)\n",
        "\n",
        "for name in unique_names:\n",
        "    # Filter par nom de chaque Sénateur\n",
        "    filtered_data = combined_df[combined_df['Nom_intervenants'] == name][['Date_seances', 'Num_page,'Titre','Texte','Thème']]\n",
        "\n",
        "    # Supprimer les lignes en double en se basant sur les colonnes 'Date_seances' et 'Num_page'\n",
        "    filtered_data = filtered_data.drop_duplicates(subset=['Date_seances', 'Num_page'])\n",
        "\n",
        "    # Supprimer 'Mme.' et 'M.' et remplacer les espaces par des underscores\n",
        "    formatted_name = name.replace(\"M.\", \"\").replace(\"Mmes\", \"\").replace(\"Mme\", \"\").replace(\" \", \"_\")\n",
        "    if formatted_name.startswith(\"_\"): # Mme Sophie Martin -> _Sophie_Martin\n",
        "        formatted_name = formatted_name[1:]\n",
        "\n",
        "    filename = f\"{formatted_name}.csv\"\n",
        "    file_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    # Ajouter un suffixe\n",
        "    if os.path.exists(file_path):\n",
        "        base, ext = os.path.splitext(filename)\n",
        "        duplicate_filename = f\"{base}_Rapporteur{ext}\"\n",
        "        file_path = os.path.join(output_dir, duplicate_filename)\n",
        "\n",
        "    filtered_data.to_csv(file_path, index=False, encoding='utf-8-sig')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYdFMZs-9-Iv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIHYyVuwD52s"
      },
      "source": [
        "# Filtre les noms par liste de Senateurs 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vKZ5e7qK38U"
      },
      "outputs": [],
      "source": [
        "\n",
        "formatted_names = [\n",
        "    \"Pascal_Allizard\",\n",
        "    \"Jean-Claude_Anglars\",\n",
        "    \"Maurice_Antiste\",\n",
        "    \"Cathy_Apourceau-Poly\",\n",
        "    \"Jean-Michel_Arnaud\",\n",
        "    \"Stéphane_Artano\",\n",
        "    \"Viviane_Artigalas\",\n",
        "    \"Éliane_Assassi\",\n",
        "    \"David_Assouline\",\n",
        "    \"Serge_Babary\",\n",
        "    \"Jérémy_Bacchi\",\n",
        "    \"Jean_Bacci\",\n",
        "    \"Jean-Pierre_Bansard\",\n",
        "    \"Julien_Bargeton\",\n",
        "    \"Philippe_Bas\",\n",
        "    \"Jérôme_Bascher\",\n",
        "    \"Arnaud_Bazin\",\n",
        "    \"Arnaud_de_Belenet\",\n",
        "    \"Bruno_Belin\",\n",
        "    \"Nadine_Bellurot\",\n",
        "    \"Catherine_Belrhiti\",\n",
        "    \"Guy_Benarroche\",\n",
        "    \"Esther_Benbassa\",\n",
        "    \"Martine_Berthet\",\n",
        "    \"Joël_Bigot\",\n",
        "    \"Christian_Bilhac\",\n",
        "    \"Annick_Billon\",\n",
        "    \"Étienne_Blanc\",\n",
        "    \"Jean-Baptiste_Blanc\",\n",
        "    \"Florence_Blatrix_Contat\",\n",
        "    \"Éric_Bocquet\",\n",
        "    \"Christine_Bonfanti-Dossat\",\n",
        "    \"François_Bonhomme\",\n",
        "    \"Bernard_Bonne\",\n",
        "    \"François_Bonneau\",\n",
        "    \"Philippe_Bonnecarrère\",\n",
        "    \"Nicole_Bonnefoy\",\n",
        "    \"Michel_Bonnus\",\n",
        "    \"Alexandra_Borchio_Fontimp\",\n",
        "    \"Denis_Bouad\",\n",
        "    \"Gilbert_Bouchet\",\n",
        "    \"Céline_Boulay-Espéronnier\",\n",
        "    \"Yves_Bouloux\",\n",
        "    \"Hussein_Bourgi\",\n",
        "    \"Toine_Bourrat\",\n",
        "    \"Jean-Marc_Boyer\",\n",
        "    \"Valérie_Boyer\",\n",
        "    \"Daniel_Breuiller\",\n",
        "    \"Isabelle_Briquet\",\n",
        "    \"Max_Brisson\",\n",
        "    \"Céline_Brulin\",\n",
        "    \"François-Noël_Buffet\",\n",
        "    \"Bernard_Buis\",\n",
        "    \"Laurent_Burgoa\",\n",
        "    \"Henri_Cabanel\",\n",
        "    \"Alain_Cadec\",\n",
        "    \"Olivier_Cadic\",\n",
        "    \"François_Calvet\",\n",
        "    \"Christian_Cambon\",\n",
        "    \"Agnès_Canayer\",\n",
        "    \"Michel_Canévet\",\n",
        "    \"Vincent_Capo-Canellas\",\n",
        "    \"Emmanuel_Capus\",\n",
        "    \"Rémi_Cardon\",\n",
        "    \"Jean-Noël_Cardoux\",\n",
        "    \"Marie-Arlette_Carlotti\",\n",
        "    \"Maryse_Carrère\",\n",
        "    \"Alain_Cazabonne\",\n",
        "    \"Samantha_Cazebonne\",\n",
        "    \"Anne_Chain-Larché\",\n",
        "    \"Patrick_Chaize\",\n",
        "    \"Yan_Chantrel\",\n",
        "    \"Pierre_Charon\",\n",
        "    \"Daniel_Chasseing\",\n",
        "    \"Alain_Chatillon\",\n",
        "    \"Patrick_Chauvet\",\n",
        "    \"Marie-Christine_Chauvin\",\n",
        "    \"Guillaume_Chevrollier\",\n",
        "    \"Marta_de_Cidrac\",\n",
        "    \"Olivier_Cigolotti\",\n",
        "    \"Laurence_Cohen\",\n",
        "    \"Catherine_Conconne\",\n",
        "    \"Hélène_Conway-Mouret\",\n",
        "    \"Jean-Pierre_Corbisez\",\n",
        "    \"Édouard_Courtial\",\n",
        "    \"Thierry_Cozic\",\n",
        "    \"Cécile_Cukierman\",\n",
        "    \"Pierre_Cuypers\",\n",
        "    \"Michel_Dagbert\",\n",
        "    \"Ronan_Dantec\",\n",
        "    \"Laure_Darcos\",\n",
        "    \"Mathieu_Darnaud\",\n",
        "    \"Marc-Philippe_Daubresse\",\n",
        "    \"Jean-Pierre_Decool\",\n",
        "    \"Vincent_Delahaye\",\n",
        "    \"Nathalie_Delattre\",\n",
        "    \"Bernard_Delcros\",\n",
        "    \"Annie_Delmont-Koropoulis\",\n",
        "    \"Patricia_Demas\",\n",
        "    \"Stéphane_Demilly\",\n",
        "    \"Michel_Dennemont\",\n",
        "    \"Catherine_Deroche\",\n",
        "    \"Chantal_Deseyne\",\n",
        "    \"Yves_Détraigne\",\n",
        "    \"Brigitte_Devésa\",\n",
        "    \"Gilbert-Luc_Devinaz\",\n",
        "    \"Catherine_Di_Folco\",\n",
        "    \"Nassimah_Dindar\",\n",
        "    \"Élisabeth_Doineau\",\n",
        "    \"Philippe_Dominati\",\n",
        "    \"Thomas_Dossus\",\n",
        "    \"Sabine_Drexler\",\n",
        "    \"Alain_Duffourg\",\n",
        "    \"Catherine_Dumas\",\n",
        "    \"Françoise_Dumont\",\n",
        "    \"Laurent_Duplomb\",\n",
        "    \"Jérôme_Durain\",\n",
        "    \"Nicole_Duranton\",\n",
        "    \"Vincent_Éblé\",\n",
        "    \"Frédérique_Espagnac\",\n",
        "    \"Dominique_Estrosi_Sassone\",\n",
        "    \"Jacqueline_Eustache-Brinio\",\n",
        "    \"Marie_Evrard\",\n",
        "    \"Gilbert_Favreau\",\n",
        "    \"Françoise_Férat\",\n",
        "    \"Rémi_Féraud\",\n",
        "    \"Corinne_Féret\",\n",
        "    \"Jacques_Fernique\",\n",
        "    \"Bernard_Fialaire\",\n",
        "    \"Jean-Luc_Fichet\",\n",
        "    \"Martine_Filleul\",\n",
        "    \"Philippe_Folliot\",\n",
        "    \"Bernard_Fournier\",\n",
        "    \"Christophe-André_Frassa\",\n",
        "    \"Pierre_Frogier\",\n",
        "    \"Amel_Gacquerre\",\n",
        "    \"Laurence_Garnier\",\n",
        "    \"Joëlle_Garriaud-Maylam\",\n",
        "    \"Françoise_Gatel\",\n",
        "    \"André_Gattolin\",\n",
        "    \"Fabien_Gay\",\n",
        "    \"Fabien_Genet\",\n",
        "    \"Frédérique_Gerbaud\",\n",
        "    \"Hervé_Gillé\",\n",
        "    \"Éric_Gold\",\n",
        "    \"Guillaume_Gontard\",\n",
        "    \"Béatrice_Gosselin\",\n",
        "    \"Nathalie_Goulet\",\n",
        "    \"Sylvie_Goy-Chavent\",\n",
        "    \"Jean-Pierre_Grand\",\n",
        "    \"Michelle_Gréaume\",\n",
        "    \"Daniel_Gremillet\",\n",
        "    \"Jacques_Grosperrin\",\n",
        "    \"Pascale_Gruny\",\n",
        "    \"Charles_Guené\",\n",
        "    \"Daniel_Gueret\",\n",
        "    \"Jean-Noël_Guérini\",\n",
        "    \"Joël_Guerriau\",\n",
        "    \"Jocelyne_Guidez\",\n",
        "    \"Véronique_Guillotin\",\n",
        "    \"André_Guiol\",\n",
        "    \"Laurence_Harribey\",\n",
        "    \"Abdallah_Hassani\",\n",
        "    \"Nadège_Havet\",\n",
        "    \"Ludovic_Haye\",\n",
        "    \"Olivier_Henno\",\n",
        "    \"Loïc_Hervé\",\n",
        "    \"Christine_Herzog\",\n",
        "    \"Jean_Hingray\",\n",
        "    \"Jean-Michel_Houllegatte\",\n",
        "    \"Alain_Houpert\",\n",
        "    \"Jean-Raymond_Hugonet\",\n",
        "    \"Jean-François_Husson\",\n",
        "    \"Xavier_Iacovelli\",\n",
        "    \"Corinne_Imbert\",\n",
        "    \"Annick_Jacquemet\",\n",
        "    \"Micheline_Jacques\",\n",
        "    \"Olivier_Jacquin\",\n",
        "    \"Jean-Marie_Janssens\",\n",
        "    \"Victoire_Jasmin\",\n",
        "    \"Éric_Jeansannetas\",\n",
        "    \"Patrice_Joly\",\n",
        "    \"Bernard_Jomier\",\n",
        "    \"Else_Joseph\",\n",
        "    \"Gisèle_Jourda\",\n",
        "    \"Muriel_Jourda\",\n",
        "    \"Alain_Joyandet\",\n",
        "    \"Patrick_Kanner\",\n",
        "    \"Roger_Karoutchi\",\n",
        "    \"Claude_Kern\",\n",
        "    \"Éric_Kerrouche\",\n",
        "    \"Christian_Klinger\",\n",
        "    \"Mikaele_Kulimoetoke\",\n",
        "    \"Marie-Pierre_de_La_Gontrie\",\n",
        "    \"Sonia_de_La_Provôté\",\n",
        "    \"Joël_Labbé\",\n",
        "    \"Laurent_Lafon\",\n",
        "    \"Jean-Louis_Lagourgue\",\n",
        "    \"Gérard_Lahellec\",\n",
        "    \"Marc_Laménie\",\n",
        "    \"Gérard_Larcher\",\n",
        "    \"Florence_Lassarade\",\n",
        "    \"Michel_Laugier\",\n",
        "    \"Daniel_Laurent\",\n",
        "    \"Pierre_Laurent\",\n",
        "    \"Christine_Lavarde\",\n",
        "    \"Jean-Yves_Leconte\",\n",
        "    \"Antoine_Lefèvre\",\n",
        "    \"Dominique_de_Legge\",\n",
        "    \"Ronan_Le_Gleut\",\n",
        "    \"Annie_Le_Houerou\",\n",
        "    \"Jean-Baptiste_Lemoyne\",\n",
        "    \"Jacques_Le_Nay\",\n",
        "    \"Olivier_Léonhardt\",\n",
        "    \"Henri_Leroy\",\n",
        "    \"Stéphane_Le_Rudulier\",\n",
        "    \"Valérie_Létard\",\n",
        "    \"Pierre-Antoine_Levi\",\n",
        "    \"Martin_Lévrier\",\n",
        "    \"Brigitte_Lherbier\",\n",
        "    \"Marie-Noëlle_Lienemann\",\n",
        "    \"Anne-Catherine_Loisier\",\n",
        "    \"Jean-François_Longeot\",\n",
        "    \"Gérard_Longuet\",\n",
        "    \"Vivette_Lopez\",\n",
        "    \"Pierre_Louault\",\n",
        "    \"Jean-Jacques_Lozach\",\n",
        "    \"Monique_Lubin\",\n",
        "    \"Victorin_Lurel\",\n",
        "    \"Jacques-Bernard_Magner\",\n",
        "    \"Viviane_Malet\",\n",
        "    \"Claude_Malhuret\",\n",
        "    \"Didier_Mandelli\",\n",
        "    \"Alain_Marc\",\n",
        "    \"Frédéric_Marchand\",\n",
        "    \"Monique_de_Marco\",\n",
        "    \"Didier_Marie\",\n",
        "    \"Hervé_Marseille\",\n",
        "    \"Pascal_Martin\",\n",
        "    \"Jean_Louis_Masson\",\n",
        "    \"Hervé_Maurey\",\n",
        "    \"Pierre_Médevielle\",\n",
        "    \"Thierry_Meignen\",\n",
        "    \"Colette_Mélot\",\n",
        "    \"Franck_Menonville\",\n",
        "    \"Marie_Mercier\",\n",
        "    \"Serge_Mérillou\",\n",
        "    \"Michelle_Meunier\",\n",
        "    \"Sébastien_Meurant\",\n",
        "    \"Jean-Jacques_Michau\",\n",
        "    \"Brigitte_Micouleau\",\n",
        "    \"Alain_Milon\",\n",
        "    \"Jean-Marie_Mizzon\",\n",
        "    \"Jean-Pierre_Moga\",\n",
        "    \"Thani_Mohamed_Soilihi\",\n",
        "    \"Marie-Pierre_Monier\",\n",
        "    \"Franck_Montaugé\",\n",
        "    \"Albéric_de_Montgolfier\",\n",
        "    \"Catherine_Morin-Desailly\",\n",
        "    \"Philippe_Mouiller\",\n",
        "    \"Laurence_Muller-Bronn\",\n",
        "    \"Philippe_Nachbar\",\n",
        "    \"Louis-Jean_de_Nicolaÿ\",\n",
        "    \"Sylviane_Noël\",\n",
        "    \"Claude_Nougein\",\n",
        "    \"Pierre_Ouzoulias\",\n",
        "    \"Olivier_Paccaud\",\n",
        "    \"Guylène_Pantel\",\n",
        "    \"Jean-Jacques_Panunzi\",\n",
        "    \"Vanina_Paoli-Gagin\",\n",
        "    \"Paul_Toussaint_Parigi\",\n",
        "    \"Georges_Patient\",\n",
        "    \"François_Patriat\",\n",
        "    \"Philippe_Paul\",\n",
        "    \"Cyril_Pellevat\",\n",
        "    \"Philippe_Pemezec\",\n",
        "    \"Cédric_Perrin\",\n",
        "    \"Évelyne_Perrot\",\n",
        "    \"Annick_Petrus\",\n",
        "    \"Marie-Laure_Phinera-Horth\",\n",
        "    \"Stéphane_Piednoir\",\n",
        "    \"Sebastien_Pla\",\n",
        "    \"Kristina_Pluchet\",\n",
        "    \"Gérard_Poadja\",\n",
        "    \"Rémy_Pointereau\",\n",
        "    \"Raymonde_Poncet_Monge\",\n",
        "    \"Émilienne_Poumirol\",\n",
        "    \"Angèle_Préville\",\n",
        "    \"Sophie_Primas\",\n",
        "    \"Jean-Paul_Prince\",\n",
        "    \"Catherine_Procaccia\",\n",
        "    \"Frédérique_Puissat\",\n",
        "    \"Daphné_Ract-Madoux\",\n",
        "    \"Isabelle_Raimond-Pavero\",\n",
        "    \"Didier_Rambaud\",\n",
        "    \"Jean-François_Rapin\",\n",
        "    \"Stéphane_Ravier\",\n",
        "    \"Claude_Raynal\",\n",
        "    \"Christian_Redon-Sarrazy\",\n",
        "    \"Damien_Regnard\",\n",
        "    \"André_Reichardt\",\n",
        "    \"Évelyne_Renaud-Garabedian\",\n",
        "    \"Jean-Claude_Requier\",\n",
        "    \"Bruno_Retailleau\",\n",
        "    \"Alain_Richard\",\n",
        "    \"Marie-Pierre_Richer\",\n",
        "    \"Olivier_Rietmann\",\n",
        "    \"Sylvie_Robert\",\n",
        "    \"Gilbert_Roger\",\n",
        "    \"Teva_Rohfritsch\",\n",
        "    \"Bruno_Rojouan\",\n",
        "    \"Laurence_Rossignol\",\n",
        "    \"Jean-Yves_Roux\",\n",
        "    \"Denise_Saint-Pé\",\n",
        "    \"Daniel_Salmon\",\n",
        "    \"Hugues_Saury\",\n",
        "    \"Stéphane_Sautarel\",\n",
        "    \"René-Paul_Savary\",\n",
        "    \"Michel_Savin\",\n",
        "    \"Pascal_Savoldelli\",\n",
        "    \"Elsa_Schalck\",\n",
        "    \"Patricia_Schillinger\",\n",
        "    \"Vincent_Segouin\",\n",
        "    \"Bruno_Sido\",\n",
        "    \"Jean_Sol\",\n",
        "    \"Nadia_Sollogoub\",\n",
        "    \"Laurent_Somon\",\n",
        "    \"Lucien_Stanzione\",\n",
        "    \"Jean-Pierre_Sueur\",\n",
        "    \"Philippe_Tabarot\",\n",
        "    \"Sophie_Taillé-Polian\",\n",
        "    \"Rachid_Temal\",\n",
        "    \"Lana_Tetuanui\",\n",
        "    \"Dominique_Théophile\",\n",
        "    \"Claudine_Thomas\",\n",
        "    \"Jean-Claude_Tissot\",\n",
        "    \"Jean-Marc_Todeschini\",\n",
        "    \"Mickaël_Vallet\",\n",
        "    \"André_Vallini\",\n",
        "    \"Sabine_Van_Heghe\",\n",
        "    \"Jean-Marie_Vanlerenberghe\",\n",
        "    \"Marie-Claude_Varaillas\",\n",
        "    \"Yannick_Vaugrenard\",\n",
        "    \"Anne_Ventalon\",\n",
        "    \"Dominique_Vérien\",\n",
        "    \"Sylvie_Vermeillet\",\n",
        "    \"Pierre-Jean_Verzelen\",\n",
        "    \"Cédric_Vial\",\n",
        "    \"Jean_Pierre_Vogel\",\n",
        "    \"Mélanie_Vogel\",\n",
        "    \"Dany_Wattebled\"\n",
        "]\n",
        "\n",
        "import shutil\n",
        "source_folder = '/content/drive/MyDrive/French_Outpu_22_9/original_data'\n",
        "\n",
        "# Dossier a les fichier CSV apres filtre\n",
        "destination_folder = '/content/drive/MyDrive/French_Outpu_22_9/filter_data'\n",
        "count = 1\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "for filename in os.listdir(source_folder):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_name_without_extension = os.path.splitext(filename)[0]\n",
        "        if file_name_without_extension in formatted_names:\n",
        "            source_file_path = os.path.join(source_folder, filename)\n",
        "            destination_file_path = os.path.join(destination_folder, filename)\n",
        "            shutil.copy2(source_file_path, destination_file_path)\n",
        "            print(f\"Đã sao chép file: {filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Q4lRB-K3_v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unqmx9YpK4Gp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
